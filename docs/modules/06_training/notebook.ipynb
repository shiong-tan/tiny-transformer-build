{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Training Pipeline - Interactive Notebook\n",
    "\n",
    "This notebook provides a hands-on walkthrough of training transformer language models.\n",
    "\n",
    "**Topics Covered:**\n",
    "1. Dataset creation and exploration\n",
    "2. Learning rate scheduling\n",
    "3. Training loop basics\n",
    "4. Checkpointing and resuming\n",
    "5. Training visualization\n",
    "6. Complete end-to-end training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent))\n",
    "\n",
    "from tiny_transformer.training import (\n",
    "    TextDataset,\n",
    "    CharTokenizer,\n",
    "    create_data_loaders,\n",
    "    WarmupCosineScheduler,\n",
    "    WarmupLinearScheduler,\n",
    "    Trainer,\n",
    "    TrainerConfig,\n",
    "    compute_perplexity,\n",
    "    set_seed\n",
    ")\n",
    "from tiny_transformer.model import TinyTransformerLM, get_model_config\n",
    "\n",
    "set_seed(42)\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Creation and Exploration\n",
    "\n",
    "We'll create a simple text dataset for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for training\n",
    "text = \"\"\"hello world this is a test of the training pipeline. \n",
    "we will train a small transformer model on this text. \n",
    "the model will learn to predict the next character given previous characters.\"\"\"\n",
    "\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"Unique characters: {len(set(text))}\")\n",
    "print(f\"\\nFirst 100 characters:\\n{text[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = CharTokenizer()\n",
    "tokenizer.fit(text)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"\\nVocabulary: {sorted(tokenizer.vocab.keys())}\")\n",
    "\n",
    "# Encode text\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"\\nTotal tokens: {len(tokens)}\")\n",
    "print(f\"First 20 tokens: {tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "seq_len = 32\n",
    "dataset = TextDataset(tokens, seq_len=seq_len)\n",
    "\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "# Get first example\n",
    "input_ids, target_ids = dataset[0]\n",
    "print(f\"\\nInput shape: {input_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "\n",
    "# Decode to verify\n",
    "input_text = tokenizer.decode(input_ids.tolist())\n",
    "target_text = tokenizer.decode(target_ids.tolist())\n",
    "\n",
    "print(f\"\\nInput:  '{input_text}'\")\n",
    "print(f\"Target: '{target_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Rate Scheduling\n",
    "\n",
    "Visualize warmup + cosine decay learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy model and optimizer\n",
    "model = nn.Linear(10, 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Create scheduler\n",
    "warmup_steps = 100\n",
    "total_steps = 1000\n",
    "peak_lr = 1e-3\n",
    "min_lr = 1e-5\n",
    "\n",
    "scheduler = WarmupCosineScheduler(\n",
    "    optimizer,\n",
    "    warmup_steps=warmup_steps,\n",
    "    total_steps=total_steps,\n",
    "    peak_lr=peak_lr,\n",
    "    min_lr=min_lr\n",
    ")\n",
    "\n",
    "# Collect learning rates\n",
    "lrs = []\n",
    "for step in range(total_steps):\n",
    "    lrs.append(scheduler.get_last_lr()[0])\n",
    "    scheduler.step()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lrs, linewidth=2)\n",
    "plt.axvline(warmup_steps, color='r', linestyle='--', label=f'End of warmup (step {warmup_steps})')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Warmup + Cosine Decay Learning Rate Schedule')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial LR: {lrs[0]:.2e}\")\n",
    "print(f\"Peak LR (after warmup): {lrs[warmup_steps]:.2e}\")\n",
    "print(f\"Final LR: {lrs[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Training Loop\n",
    "\n",
    "Demonstrate a simple training loop on our tiny dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, _ = create_data_loaders(\n",
    "    train_tokens=tokens,\n",
    "    val_tokens=None,\n",
    "    seq_len=32,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(f\"Train loader batches: {len(train_loader)}\")\n",
    "print(f\"Samples per batch: {train_loader.batch_size}\")\n",
    "\n",
    "# Get first batch\n",
    "for input_ids, target_ids in train_loader:\n",
    "    print(f\"\\nBatch input shape: {input_ids.shape}\")\n",
    "    print(f\"Batch target shape: {target_ids.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tiny model\n",
    "tiny_config = get_model_config('tiny')\n",
    "model = TinyTransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    **tiny_config\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model config: d_model={tiny_config['d_model']}, n_heads={tiny_config['n_heads']}, n_layers={tiny_config['n_layers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual training loop (for illustration)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "losses = []\n",
    "num_steps = 50\n",
    "\n",
    "print(\"Training for 50 steps...\")\n",
    "for step in range(num_steps):\n",
    "    for input_ids, target_ids in train_loader:\n",
    "        # Forward pass\n",
    "        logits, _ = model(input_ids)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            target_ids.view(-1)\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            perplexity = compute_perplexity(loss.item())\n",
    "            print(f\"Step {step:3d} | Loss: {loss.item():.4f} | Perplexity: {perplexity:.2f}\")\n",
    "        \n",
    "        break  # One batch per step\n",
    "\n",
    "print(\"\\n✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using the Trainer Class\n",
    "\n",
    "Use the high-level Trainer interface for production training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh model and data\n",
    "set_seed(42)\n",
    "model = TinyTransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    **tiny_config\n",
    ")\n",
    "\n",
    "train_loader, _ = create_data_loaders(\n",
    "    train_tokens=tokens,\n",
    "    val_tokens=None,\n",
    "    seq_len=32,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "# Configure trainer\n",
    "config = TrainerConfig(\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    grad_clip=1.0,\n",
    "    log_interval=10,\n",
    "    max_steps=100,\n",
    "    warmup_steps=20,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"Trainer configured\")\n",
    "print(f\"Optimizer: {type(trainer.optimizer).__name__}\")\n",
    "print(f\"Scheduler: {type(trainer.scheduler).__name__}\")\n",
    "print(f\"Device: {trainer.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using Trainer\n",
    "final_metrics = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final loss: {final_metrics['loss']:.4f}\")\n",
    "print(f\"Final perplexity: {final_metrics['perplexity']:.2f}\")\n",
    "print(f\"Total steps: {trainer.step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checkpointing\n",
    "\n",
    "Save and load model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "checkpoint_path = \"temp_checkpoint.pt\"\n",
    "trainer.save_checkpoint(checkpoint_path)\n",
    "\n",
    "print(f\"✓ Checkpoint saved to {checkpoint_path}\")\n",
    "print(f\"\\nCheckpoint contains:\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "for key in checkpoint.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint into new model\n",
    "new_model = TinyTransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    **tiny_config\n",
    ")\n",
    "\n",
    "new_trainer = Trainer(\n",
    "    model=new_model,\n",
    "    train_loader=train_loader,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "new_trainer.load_checkpoint(checkpoint_path)\n",
    "\n",
    "print(f\"✓ Checkpoint loaded\")\n",
    "print(f\"Resumed at step: {new_trainer.step}\")\n",
    "print(f\"Resumed at epoch: {new_trainer.epoch}\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove(checkpoint_path)\n",
    "print(f\"✓ Temporary checkpoint removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Metrics Visualization\n",
    "\n",
    "Visualize loss, perplexity, and learning rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training with metrics collection\n",
    "set_seed(42)\n",
    "model = TinyTransformerLM(vocab_size=tokenizer.vocab_size, **tiny_config)\n",
    "train_loader, _ = create_data_loaders(tokens, None, 32, 4)\n",
    "\n",
    "config = TrainerConfig(\n",
    "    learning_rate=1e-3,\n",
    "    max_steps=200,\n",
    "    warmup_steps=40,\n",
    "    log_interval=10,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Manual training with metric tracking\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "scheduler = WarmupCosineScheduler(\n",
    "    optimizer, \n",
    "    warmup_steps=40, \n",
    "    total_steps=200, \n",
    "    peak_lr=1e-3, \n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "lrs = []\n",
    "steps = []\n",
    "\n",
    "print(\"Training for 200 steps with metric collection...\")\n",
    "step = 0\n",
    "while step < 200:\n",
    "    for input_ids, target_ids in train_loader:\n",
    "        logits, _ = model(input_ids)\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            target_ids.view(-1)\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "        steps.append(step)\n",
    "        \n",
    "        step += 1\n",
    "        if step >= 200:\n",
    "            break\n",
    "\n",
    "print(\"✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(steps, losses, linewidth=2, color='blue')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[0, 1].plot(steps, lrs, linewidth=2, color='green')\n",
    "axes[0, 1].axvline(40, color='r', linestyle='--', alpha=0.5, label='End of warmup')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Learning Rate')\n",
    "axes[0, 1].set_title('Learning Rate Schedule')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "perplexities = [compute_perplexity(l) for l in losses]\n",
    "axes[1, 0].plot(steps, perplexities, linewidth=2, color='orange')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Perplexity')\n",
    "axes[1, 0].set_title('Perplexity')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Smoothed loss (moving average)\n",
    "window = 10\n",
    "smoothed_loss = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "axes[1, 1].plot(steps[:len(smoothed_loss)], smoothed_loss, linewidth=2, color='purple')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Smoothed Loss')\n",
    "axes[1, 1].set_title(f'Loss (Moving Average, window={window})')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial loss: {losses[0]:.4f} → Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"  Initial perplexity: {perplexities[0]:.2f} → Final perplexity: {perplexities[-1]:.2f}\")\n",
    "print(f\"  Loss reduction: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "1. **Dataset Creation** - Using TextDataset and CharTokenizer for character-level language modeling\n",
    "2. **Learning Rate Scheduling** - Warmup + cosine decay for stable training\n",
    "3. **Training Loops** - Both manual and Trainer-based approaches\n",
    "4. **Checkpointing** - Saving and resuming training state\n",
    "5. **Metrics Visualization** - Loss, perplexity, and learning rate tracking\n",
    "\n",
    "**Next Steps:**\n",
    "- Try the exercises in `exercises/exercises.py`\n",
    "- Train on larger datasets (Shakespeare, WikiText)\n",
    "- Experiment with different hyperparameters\n",
    "- Move on to Module 07 (Sampling & Generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
