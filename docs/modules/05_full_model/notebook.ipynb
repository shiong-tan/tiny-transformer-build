{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Full Transformer Language Model\n",
    "\n",
    "This notebook explores the complete **TinyTransformerLM** model that integrates all components:\n",
    "- Token and positional embeddings\n",
    "- Stack of transformer blocks  \n",
    "- Language modeling head\n",
    "- Weight tying\n",
    "- Text generation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. How all transformer components integrate into a complete language model\n",
    "2. The weight tying mechanism and its benefits\n",
    "3. Parameter counting and model scaling laws\n",
    "4. How to analyze model architecture and memory requirements\n",
    "5. Basic text generation with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '../../..')\n",
    "\n",
    "from tiny_transformer.model import TinyTransformerLM, get_model_config\n",
    "from tiny_transformer.embeddings import TransformerEmbedding\n",
    "from tiny_transformer.transformer_block import TransformerBlock\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Architecture Overview\n",
    "\n",
    "Let's visualize the complete transformer architecture:\n",
    "\n",
    "```\n",
    "Input Tokens (B, T)\n",
    "    ↓\n",
    "Embedding Layer: TransformerEmbedding\n",
    "    ├─ Token Embedding (vocab_size → d_model)\n",
    "    ├─ Positional Encoding (sinusoidal or learned)\n",
    "    └─ Dropout\n",
    "    ↓\n",
    "Transformer Blocks (n_layers)\n",
    "    ├─ Block 1: MultiHeadAttention + FeedForward\n",
    "    ├─ Block 2: MultiHeadAttention + FeedForward  \n",
    "    ├─ ...\n",
    "    └─ Block N: MultiHeadAttention + FeedForward\n",
    "    ↓\n",
    "Final Layer Norm\n",
    "    ↓\n",
    "Language Modeling Head (d_model → vocab_size)\n",
    "    ↓\n",
    "Output Logits (B, T, vocab_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small model for exploration\n",
    "model = TinyTransformerLM(\n",
    "    vocab_size=1000,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=3,\n",
    "    d_ff=512,\n",
    "    max_len=256,\n",
    "    dropout=0.1,\n",
    "    tie_weights=True\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "print(model)\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal parameters: {model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating Models with Preset Configurations\n",
    "\n",
    "The library provides preset configurations for different model sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore all preset configurations\n",
    "sizes = [\"tiny\", \"small\", \"medium\", \"large\"]\n",
    "vocab_size = 10000\n",
    "\n",
    "configs = {}\n",
    "models = {}\n",
    "\n",
    "for size in sizes:\n",
    "    config = get_model_config(size)\n",
    "    configs[size] = config\n",
    "    \n",
    "    model = TinyTransformerLM(vocab_size=vocab_size, **config)\n",
    "    models[size] = model\n",
    "    \n",
    "    params = model.count_parameters()\n",
    "    \n",
    "    print(f\"{size.upper():8s} | d_model={config['d_model']:4d} | \"\n",
    "          f\"n_heads={config['n_heads']:2d} | n_layers={config['n_layers']:2d} | \"\n",
    "          f\"params={params/1e6:6.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parameter Analysis\n",
    "\n",
    "Let's analyze where parameters are distributed in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameter breakdown for tiny model\n",
    "model_tiny = models[\"tiny\"]\n",
    "breakdown = model_tiny.get_parameter_breakdown()\n",
    "\n",
    "print(\"Parameter Breakdown (Tiny Model):\")\n",
    "print(\"=\" * 60)\n",
    "for component, count in breakdown.items():\n",
    "    if component != 'total':\n",
    "        pct = (count / breakdown['total']) * 100\n",
    "        print(f\"{component:25s}: {count:>10,} ({pct:>5.1f}%)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'TOTAL':25s}: {breakdown['total']:>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Filter out 'total'\n",
    "components = [k for k in breakdown.keys() if k != 'total']\n",
    "counts = [breakdown[k] for k in components]\n",
    "\n",
    "# Create pie chart\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
    "ax.pie(counts, labels=components, autopct='%1.1f%%', \n",
    "       colors=colors, startangle=90)\n",
    "ax.set_title(f'Parameter Distribution\\n({breakdown[\"total\"]:,} total parameters)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Transformer blocks contain most parameters\")\n",
    "print(\"- Embeddings take significant space (vocab_size × d_model)\")\n",
    "print(\"- Weight tying reduces lm_head contribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Weight Tying Mechanism\n",
    "\n",
    "Weight tying shares parameters between the token embedding and language modeling head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with and without weight tying\n",
    "config = get_model_config(\"small\")\n",
    "\n",
    "model_tied = TinyTransformerLM(vocab_size=10000, tie_weights=True, **config)\n",
    "model_untied = TinyTransformerLM(vocab_size=10000, tie_weights=False, **config)\n",
    "\n",
    "params_tied = model_tied.count_parameters()\n",
    "params_untied = model_untied.count_parameters()\n",
    "difference = params_untied - params_tied\n",
    "reduction_pct = (difference / params_untied) * 100\n",
    "\n",
    "print(\"Weight Tying Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"With weight tying:    {params_tied:>12,} parameters\")\n",
    "print(f\"Without weight tying: {params_untied:>12,} parameters\")\n",
    "print(f\"Difference:           {difference:>12,} parameters\")\n",
    "print(f\"Reduction:            {reduction_pct:>12.1f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify weight sharing\n",
    "print(\"\\nWeight Sharing Verification:\")\n",
    "emb_weight = model_tied.embedding.token_embedding.embedding.weight\n",
    "lm_weight = model_tied.lm_head.weight\n",
    "\n",
    "print(f\"Embedding weight shape: {emb_weight.shape}\")\n",
    "print(f\"LM head weight shape:   {lm_weight.shape}\")\n",
    "print(f\"Same tensor? {emb_weight is lm_weight}\")\n",
    "print(f\"Same memory address? {emb_weight.data_ptr() == lm_weight.data_ptr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Forward Pass Walkthrough\n",
    "\n",
    "Let's trace a forward pass through the model step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input\n",
    "model_tiny.eval()\n",
    "batch_size, seq_len = 4, 16\n",
    "tokens = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "\n",
    "print(f\"Input tokens shape: {tokens.shape}\")\n",
    "print(f\"Sample tokens: {tokens[0, :10].tolist()}\")\n",
    "print()\n",
    "\n",
    "# Step 1: Embedding\n",
    "with torch.no_grad():\n",
    "    embedded = model_tiny.embedding(tokens)\n",
    "    print(f\"After embedding: {embedded.shape}\")\n",
    "    print(f\"Embedding mean: {embedded.mean():.4f}, std: {embedded.std():.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Through transformer blocks\n",
    "    x = embedded\n",
    "    for i, block in enumerate(model_tiny.blocks):\n",
    "        x_before = x.clone()\n",
    "        x, attn = block(x)\n",
    "        \n",
    "        print(f\"Block {i+1}: {x.shape}\")\n",
    "        print(f\"  Mean: {x.mean():.4f}, Std: {x.std():.4f}\")\n",
    "        print(f\"  Change from previous: {(x - x_before).abs().mean():.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Final layer norm\n",
    "    x = model_tiny.ln_f(x)\n",
    "    print(f\"After final LayerNorm: {x.shape}\")\n",
    "    print(f\"Mean: {x.mean():.4f}, Std: {x.std():.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Language modeling head\n",
    "    logits = model_tiny.lm_head(x)\n",
    "    print(f\"After LM head: {logits.shape}\")\n",
    "    print(f\"Logit mean: {logits.mean():.4f}, std: {logits.std():.4f}\")\n",
    "    print(f\"Logit range: [{logits.min():.2f}, {logits.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Hidden State Analysis\n",
    "\n",
    "Extract and analyze hidden states from each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden states\n",
    "with torch.no_grad():\n",
    "    logits, hidden_states = model_tiny(tokens, return_hidden_states=True)\n",
    "\n",
    "print(f\"Number of hidden states: {len(hidden_states)}\")\n",
    "print(f\"(Embedding output + {len(model_tiny.blocks)} block outputs)\")\n",
    "print()\n",
    "\n",
    "# Analyze hidden state statistics\n",
    "for i, h in enumerate(hidden_states):\n",
    "    layer_name = \"Embedding\" if i == 0 else f\"Block {i}\"\n",
    "    print(f\"{layer_name:12s}: mean={h.mean():.4f}, std={h.std():.4f}, \"\n",
    "          f\"norm={h.norm(dim=-1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hidden state evolution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Extract statistics\n",
    "means = [h.mean().item() for h in hidden_states]\n",
    "stds = [h.std().item() for h in hidden_states]\n",
    "norms = [h.norm(dim=-1).mean().item() for h in hidden_states]\n",
    "\n",
    "layers = list(range(len(hidden_states)))\n",
    "layer_labels = [\"Emb\"] + [f\"B{i}\" for i in range(1, len(hidden_states))]\n",
    "\n",
    "# Plot mean\n",
    "axes[0].plot(layers, means, marker='o', linewidth=2)\n",
    "axes[0].set_xlabel('Layer')\n",
    "axes[0].set_ylabel('Mean Activation')\n",
    "axes[0].set_title('Hidden State Mean')\n",
    "axes[0].set_xticks(layers)\n",
    "axes[0].set_xticklabels(layer_labels)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot std\n",
    "axes[1].plot(layers, stds, marker='s', color='orange', linewidth=2)\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Standard Deviation')\n",
    "axes[1].set_title('Hidden State Std')\n",
    "axes[1].set_xticks(layers)\n",
    "axes[1].set_xticklabels(layer_labels)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot norm\n",
    "axes[2].plot(layers, norms, marker='^', color='green', linewidth=2)\n",
    "axes[2].set_xlabel('Layer')\n",
    "axes[2].set_ylabel('L2 Norm')\n",
    "axes[2].set_title('Hidden State Norm')\n",
    "axes[2].set_xticks(layers)\n",
    "axes[2].set_xticklabels(layer_labels)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- LayerNorm keeps activations normalized across layers\")\n",
    "print(\"- Residual connections preserve information flow\")\n",
    "print(\"- Deeper layers show refined representations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Model Scaling Analysis\n",
    "\n",
    "How do parameters scale with model size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect scaling data\n",
    "scaling_data = []\n",
    "\n",
    "for size in sizes:\n",
    "    model = models[size]\n",
    "    config = configs[size]\n",
    "    breakdown = model.get_parameter_breakdown()\n",
    "    \n",
    "    scaling_data.append({\n",
    "        'size': size,\n",
    "        'd_model': config['d_model'],\n",
    "        'n_layers': config['n_layers'],\n",
    "        'n_heads': config['n_heads'],\n",
    "        'd_ff': config['d_ff'],\n",
    "        'total_params': breakdown['total'],\n",
    "        'embedding_params': breakdown['embedding'],\n",
    "        'block_params': breakdown['transformer_blocks'],\n",
    "    })\n",
    "\n",
    "# Print scaling table\n",
    "print(\"Model Scaling:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Size':8s} | {'d_model':7s} | {'Layers':6s} | {'Heads':5s} | \"\n",
    "      f\"{'Total (M)':>10s} | {'Emb (M)':>8s} | {'Blocks (M)':>10s}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for data in scaling_data:\n",
    "    print(f\"{data['size']:8s} | {data['d_model']:7d} | {data['n_layers']:6d} | \"\n",
    "          f\"{data['n_heads']:5d} | {data['total_params']/1e6:>10.2f} | \"\n",
    "          f\"{data['embedding_params']/1e6:>8.2f} | \"\n",
    "          f\"{data['block_params']/1e6:>10.2f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter scaling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "size_labels = [d['size'] for d in scaling_data]\n",
    "total_params = [d['total_params'] / 1e6 for d in scaling_data]\n",
    "emb_params = [d['embedding_params'] / 1e6 for d in scaling_data]\n",
    "block_params = [d['block_params'] / 1e6 for d in scaling_data]\n",
    "\n",
    "# Stacked bar chart\n",
    "x = np.arange(len(size_labels))\n",
    "axes[0].bar(x, emb_params, label='Embeddings', color='#ff9999')\n",
    "axes[0].bar(x, block_params, bottom=emb_params, label='Transformer Blocks', \n",
    "            color='#66b3ff')\n",
    "axes[0].set_xlabel('Model Size')\n",
    "axes[0].set_ylabel('Parameters (M)')\n",
    "axes[0].set_title('Parameter Breakdown by Component')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(size_labels)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scaling curve\n",
    "d_models = [d['d_model'] for d in scaling_data]\n",
    "axes[1].plot(d_models, total_params, marker='o', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('d_model')\n",
    "axes[1].set_ylabel('Total Parameters (M)')\n",
    "axes[1].set_title('Parameter Scaling with d_model')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "for i, (d, p, s) in enumerate(zip(d_models, total_params, size_labels)):\n",
    "    axes[1].annotate(s, (d, p), textcoords=\"offset points\", \n",
    "                     xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScaling Observations:\")\n",
    "print(f\"- Parameters scale approximately as O(d_model²) due to attention\")\n",
    "print(f\"- Transformer blocks dominate for large models\")\n",
    "print(f\"- From tiny to large: {total_params[-1]/total_params[0]:.1f}× increase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Basic Text Generation\n",
    "\n",
    "Test the model's generation capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "model_tiny.eval()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Start tokens\n",
    "start_tokens = torch.randint(0, 1000, (3, 5))  # 3 sequences\n",
    "max_new_tokens = 15\n",
    "\n",
    "print(\"Input sequences:\")\n",
    "for i, seq in enumerate(start_tokens):\n",
    "    print(f\"  Seq {i+1}: {seq.tolist()}\")\n",
    "print()\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    generated = model_tiny.generate(\n",
    "        start_tokens, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "print(f\"Generated sequences (length {generated.size(1)}):\")\n",
    "for i, seq in enumerate(generated):\n",
    "    original = seq[:5].tolist()\n",
    "    new = seq[5:].tolist()\n",
    "    print(f\"  Seq {i+1}: {original} + {new}\")\n",
    "\n",
    "print(f\"\\nShape: {generated.shape}\")\n",
    "print(f\"Expected: (3, {5 + max_new_tokens})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperatures\n",
    "temperatures = [0.5, 1.0, 2.0]\n",
    "start_single = torch.randint(0, 1000, (1, 5))\n",
    "\n",
    "print(\"Generation with different temperatures:\")\n",
    "print(f\"Start sequence: {start_single[0].tolist()}\")\n",
    "print()\n",
    "\n",
    "for temp in temperatures:\n",
    "    torch.manual_seed(100)  # Same seed for comparison\n",
    "    with torch.no_grad():\n",
    "        gen = model_tiny.generate(\n",
    "            start_single,\n",
    "            max_new_tokens=10,\n",
    "            temperature=temp\n",
    "        )\n",
    "    \n",
    "    new_tokens = gen[0, 5:].tolist()\n",
    "    print(f\"T={temp:.1f}: {new_tokens}\")\n",
    "\n",
    "print()\n",
    "print(\"Notes:\")\n",
    "print(\"- Lower temperature → more deterministic\")\n",
    "print(\"- Higher temperature → more random\")\n",
    "print(\"- Temperature controls diversity of outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Memory Estimation\n",
    "\n",
    "Estimate memory requirements for different model sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_memory(model, dtype=torch.float32):\n",
    "    \"\"\"Estimate model memory in MB.\"\"\"\n",
    "    bytes_per_param = 4 if dtype == torch.float32 else 2  # fp32 vs fp16\n",
    "    \n",
    "    # Model parameters\n",
    "    num_params = model.count_parameters()\n",
    "    param_memory = num_params * bytes_per_param / (1024 ** 2)  # MB\n",
    "    \n",
    "    # Gradients (during training)\n",
    "    grad_memory = param_memory  # Same size as parameters\n",
    "    \n",
    "    # Optimizer states (AdamW has 2× params for momentum and variance)\n",
    "    optimizer_memory = 2 * param_memory\n",
    "    \n",
    "    return {\n",
    "        'parameters': param_memory,\n",
    "        'gradients': grad_memory,\n",
    "        'optimizer': optimizer_memory,\n",
    "        'total_training': param_memory + grad_memory + optimizer_memory,\n",
    "        'inference_only': param_memory\n",
    "    }\n",
    "\n",
    "# Estimate for all model sizes\n",
    "print(\"Memory Estimation (FP32):\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Size':8s} | {'Params':>10s} | {'Inference':>10s} | \"\n",
    "      f\"{'Training':>10s} | {'Total':>10s}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for size in sizes:\n",
    "    model = models[size]\n",
    "    mem = estimate_model_memory(model)\n",
    "    \n",
    "    print(f\"{size:8s} | {model.count_parameters()/1e6:>9.2f}M | \"\n",
    "          f\"{mem['inference_only']:>9.1f}MB | \"\n",
    "          f\"{mem['total_training']:>9.1f}MB | \"\n",
    "          f\"{mem['total_training']/1024:>9.2f}GB\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(\"\\nNote: This excludes activation memory which depends on batch size!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_activation_memory(model, batch_size, seq_len, dtype=torch.float32):\n",
    "    \"\"\"Estimate activation memory for a forward pass.\"\"\"\n",
    "    bytes_per_element = 4 if dtype == torch.float32 else 2\n",
    "    \n",
    "    d_model = model.d_model\n",
    "    n_layers = model.n_layers\n",
    "    n_heads = model.n_heads\n",
    "    d_ff = model.d_ff\n",
    "    \n",
    "    # Embedding output\n",
    "    emb_mem = batch_size * seq_len * d_model * bytes_per_element\n",
    "    \n",
    "    # Per transformer block:\n",
    "    # - Attention: Q, K, V, attention scores, attention output\n",
    "    qkv_mem = 3 * batch_size * seq_len * d_model * bytes_per_element\n",
    "    attn_scores_mem = batch_size * n_heads * seq_len * seq_len * bytes_per_element\n",
    "    attn_out_mem = batch_size * seq_len * d_model * bytes_per_element\n",
    "    \n",
    "    # - Feed-forward: intermediate activation\n",
    "    ff_mem = batch_size * seq_len * d_ff * bytes_per_element\n",
    "    \n",
    "    # - Residuals and layer norms\n",
    "    residual_mem = 2 * batch_size * seq_len * d_model * bytes_per_element\n",
    "    \n",
    "    block_mem = qkv_mem + attn_scores_mem + attn_out_mem + ff_mem + residual_mem\n",
    "    total_block_mem = block_mem * n_layers\n",
    "    \n",
    "    total_mem = (emb_mem + total_block_mem) / (1024 ** 2)  # MB\n",
    "    \n",
    "    return total_mem\n",
    "\n",
    "# Estimate activation memory for different batch sizes\n",
    "batch_sizes = [1, 4, 16, 32]\n",
    "seq_length = 512\n",
    "\n",
    "print(f\"\\nActivation Memory (seq_len={seq_length}, FP32):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':8s} | \" + \" | \".join([f\"B={b:2d}\" for b in batch_sizes]))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for size in [\"tiny\", \"small\"]:\n",
    "    model = models[size]\n",
    "    mems = [estimate_activation_memory(model, b, seq_length) for b in batch_sizes]\n",
    "    mem_str = \" | \".join([f\"{m:>6.1f}MB\" for m in mems])\n",
    "    print(f\"{size:8s} | {mem_str}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"- Activation memory scales linearly with batch size\")\n",
    "print(\"- Attention scores dominate memory (O(seq_len²))\")\n",
    "print(\"- Total GPU memory = Model + Gradients + Optimizer + Activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### Architecture\n",
    "1. **Complete Integration**: TinyTransformerLM combines embeddings, transformer blocks, and LM head\n",
    "2. **Pre-LN Architecture**: Layer normalization before sub-layers for stability\n",
    "3. **Causal Masking**: Lower-triangular mask prevents looking ahead\n",
    "\n",
    "### Weight Tying\n",
    "1. **Mechanism**: Share weights between token embedding and LM head\n",
    "2. **Benefits**: 15-20% parameter reduction with similar performance\n",
    "3. **Implementation**: Direct weight reference (same tensor)\n",
    "\n",
    "### Parameter Scaling\n",
    "1. **Dominance**: Transformer blocks contain most parameters (~70-80%)\n",
    "2. **Scaling**: Parameters grow as O(d_model²) and O(n_layers)\n",
    "3. **Trade-off**: More parameters → better capacity but more memory\n",
    "\n",
    "### Memory Requirements\n",
    "1. **Training**: Model + gradients + optimizer states (4× model size)\n",
    "2. **Inference**: Just model parameters (1× model size)\n",
    "3. **Activations**: Depend on batch_size × seq_len (O(seq_len²) attention)\n",
    "\n",
    "### Generation\n",
    "1. **Autoregressive**: Generate one token at a time\n",
    "2. **Temperature**: Controls randomness vs determinism\n",
    "3. **Context Window**: Limited by max_len (truncate if exceeded)\n",
    "\n",
    "### Next Steps\n",
    "- **Module 06**: Training the model (optimizers, loss, data loading)\n",
    "- **Module 07**: Advanced generation (top-k, nucleus sampling, beam search)\n",
    "- **Module 08**: Scaling to larger models and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try the following:\n",
    "1. Create a custom model configuration between \"small\" and \"medium\"\n",
    "2. Analyze its parameter breakdown\n",
    "3. Compare weight tying vs no weight tying\n",
    "4. Estimate memory requirements for batch_size=64, seq_len=1024\n",
    "5. Generate text with different temperatures and compare outputs\n",
    "\n",
    "See `exercises/exercises.py` for more structured exercises!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
