{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Attention Mechanism - Interactive Exploration\n",
    "\n",
    "Welcome to the interactive attention notebook! Here you'll:\n",
    "\n",
    "1. **Visualize** attention patterns in action\n",
    "2. **Experiment** with different inputs and see how attention responds\n",
    "3. **Understand** each component through hands-on exploration\n",
    "4. **Debug** common issues interactively\n",
    "5. **Build intuition** through experimentation\n",
    "\n",
    "**Prerequisites:**\n",
    "- Read `theory.md` for conceptual understanding\n",
    "- Try `exercises.py` to practice implementation\n",
    "- Have PyTorch installed\n",
    "\n",
    "**Time:** 60-90 minutes\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Import our attention implementation\n",
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from tiny_transformer.attention import scaled_dot_product_attention, create_causal_mask\n",
    "from tiny_transformer.utils import plot_attention_pattern, check_shape\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Components\n",
    "\n",
    "### 1.1 Query, Key, Value - What Are They?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple example with small dimensions for clarity\n",
    "batch_size = 1\n",
    "seq_len = 4  # Short sequence to visualize\n",
    "d_k = 8      # Small dimension\n",
    "\n",
    "# Create Q, K, V\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "print(f\"Query shape: {Q.shape}  (batch, positions, d_k)\")\n",
    "print(f\"Key shape:   {K.shape}  (batch, positions, d_k)\")\n",
    "print(f\"Value shape: {V.shape}  (batch, positions, d_k)\")\n",
    "\n",
    "print(\"\\nThink of it like a database:\")\n",
    "print(\"  Query  = what you're searching for\")\n",
    "print(\"  Key    = index/metadata of each item\")\n",
    "print(\"  Value  = actual content of each item\")\n",
    "print(\"\\nAttention finds which Keys match your Queries,\")\n",
    "print(\"then returns a weighted combination of the corresponding Values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Step 1: Computing Attention Scores (Q @ K^T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute similarity scores\n",
    "scores = Q @ K.transpose(-2, -1)  # (batch, seq_len, d_k) @ (batch, d_k, seq_len)\n",
    "\n",
    "print(f\"Scores shape: {scores.shape}  (batch, queries, keys)\")\n",
    "print(\"\\nEach position (query) has a score for every other position (key)\")\n",
    "print(f\"\\nRaw scores for first batch:\\n{scores[0]}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores[0].detach().numpy(), cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Score')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Raw Attention Scores (Q @ K^T)')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  Brighter = higher similarity\")\n",
    "print(\"  Each row shows how query i relates to all keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Step 2: Scaling (Divide by âˆšd_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scaled vs unscaled\n",
    "scores_unscaled = Q @ K.transpose(-2, -1)\n",
    "scores_scaled = scores_unscaled / math.sqrt(d_k)\n",
    "\n",
    "print(f\"Before scaling - Mean: {scores_unscaled.mean():.4f}, Std: {scores_unscaled.std():.4f}\")\n",
    "print(f\"After scaling  - Mean: {scores_scaled.mean():.4f}, Std: {scores_scaled.std():.4f}\")\n",
    "\n",
    "# Effect on softmax\n",
    "attn_unscaled = F.softmax(scores_unscaled[0], dim=-1)\n",
    "attn_scaled = F.softmax(scores_scaled[0], dim=-1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "im1 = ax1.imshow(attn_unscaled.detach().numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "ax1.set_title('Attention WITHOUT Scaling')\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "im2 = ax2.imshow(attn_scaled.detach().numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "ax2.set_title('Attention WITH Scaling')\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWhy scaling matters:\")\n",
    "print(\"  Unscaled: Softmax becomes too 'peaky' (one-hot like)\")\n",
    "print(\"  Scaled: More balanced attention distribution\")\n",
    "print(\"  This is crucial for gradient flow during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Step 3: Softmax (Convert to Probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax\n",
    "attention_weights = F.softmax(scores_scaled, dim=-1)\n",
    "\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nFirst query's attention distribution:\")\n",
    "print(attention_weights[0, 0])\n",
    "print(f\"\\nSum: {attention_weights[0, 0].sum():.6f} (should be 1.0)\")\n",
    "\n",
    "# Visualize all attention patterns\n",
    "plot_attention_pattern(\n",
    "    attention_weights[0],\n",
    "    title=\"Attention Weights (After Softmax)\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey properties:\")\n",
    "print(\"  âœ“ All values between 0 and 1\")\n",
    "print(\"  âœ“ Each row sums to 1 (probability distribution)\")\n",
    "print(\"  âœ“ Shows 'how much' each query attends to each key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Step 4: Apply Attention to Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted sum of values\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(f\"Output shape: {output.shape}  (same as Q, K, V)\")\n",
    "print(\"\\nWhat happened:\")\n",
    "print(\"  For each query position, we computed a weighted average of ALL values\")\n",
    "print(\"  Weights determined by attention (how much each key matches the query)\")\n",
    "\n",
    "# Demonstrate for first position\n",
    "pos = 0\n",
    "print(f\"\\nPosition {pos}:\")\n",
    "print(f\"  Attention weights: {attention_weights[0, pos]}\")\n",
    "print(f\"  Output: weighted combo of all 4 value vectors\")\n",
    "\n",
    "# Verify manually\n",
    "manual_output = torch.zeros(d_k)\n",
    "for i in range(seq_len):\n",
    "    manual_output += attention_weights[0, pos, i] * V[0, i]\n",
    "\n",
    "print(f\"\\nManual calculation matches: {torch.allclose(manual_output, output[0, pos])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Complete Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our implementation\n",
    "output, attention = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shapes:  Q={Q.shape}, K={K.shape}, V={V.shape}\")\n",
    "print(f\"Output shapes: output={output.shape}, attention={attention.shape}\")\n",
    "\n",
    "# Visualize the attention pattern\n",
    "plot_attention_pattern(\n",
    "    attention[0],\n",
    "    title=\"Scaled Dot-Product Attention Pattern\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Complete attention mechanism in one function call!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Causal Masking for Autoregressive Models\n",
    "\n",
    "In language models, we need to prevent attending to future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create causal mask\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal mask (1=allowed, 0=blocked):\")\n",
    "print(mask)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(mask.numpy(), cmap='RdYlGn', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Allowed')\n",
    "plt.xlabel('Key Position (past â†’ future)')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Causal Mask: Lower Triangular')\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        plt.text(j, i, int(mask[i, j].item()), \n",
    "                ha='center', va='center', color='black', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  Position 0 can only attend to itself (first token)\")\n",
    "print(\"  Position 1 can attend to positions 0, 1 (past and present)\")\n",
    "print(\"  Position 2 can attend to positions 0, 1, 2\")\n",
    "print(\"  etc.\")\n",
    "print(\"\\n  â†’ Prevents 'cheating' by looking at future tokens!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply causal masking to attention\n",
    "output_masked, attention_masked = scaled_dot_product_attention(Q, K, V, mask=mask.unsqueeze(0))\n",
    "\n",
    "# Compare masked vs unmasked\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "im1 = ax1.imshow(attention[0].detach().numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "ax1.set_title('Without Causal Mask')\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "im2 = ax2.imshow(attention_masked[0].detach().numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "ax2.set_title('With Causal Mask')\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice:\")\n",
    "print(\"  Right plot has upper triangle = 0 (blocked)\")\n",
    "print(\"  Attention only flows backwards and to current position\")\n",
    "print(\"  This is essential for autoregressive generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Experimenting with Different Inputs\n",
    "\n",
    "### 4.1 What Happens with Identical Keys?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Q with one query different from others\n",
    "Q_exp = torch.randn(1, 4, 8)\n",
    "\n",
    "# Make all keys identical\n",
    "K_identical = torch.ones(1, 4, 8)  # All the same!\n",
    "V_exp = torch.randn(1, 4, 8)\n",
    "\n",
    "output_exp, attention_exp = scaled_dot_product_attention(Q_exp, K_identical, V_exp)\n",
    "\n",
    "print(\"Attention with identical keys:\")\n",
    "print(attention_exp[0])\n",
    "\n",
    "plot_attention_pattern(attention_exp[0], title=\"Identical Keys â†’ Uniform Attention\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  When all keys are the same, attention is uniform (equal weights)\")\n",
    "print(\"  Makes sense: nothing distinguishes one position from another!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 What About One Very Similar Query-Key Pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scenario where Q[0] and K[2] are very similar\n",
    "Q_similar = torch.randn(1, 4, 8)\n",
    "K_similar = torch.randn(1, 4, 8)\n",
    "\n",
    "# Make Q[0] â‰ˆ K[2]\n",
    "K_similar[0, 2] = Q_similar[0, 0] + 0.1 * torch.randn(8)  # Almost identical\n",
    "\n",
    "V_similar = torch.randn(1, 4, 8)\n",
    "\n",
    "output_similar, attention_similar = scaled_dot_product_attention(Q_similar, K_similar, V_similar)\n",
    "\n",
    "print(\"Attention when Q[0] â‰ˆ K[2]:\")\n",
    "print(attention_similar[0])\n",
    "print(f\"\\nQuery 0 attention to Key 2: {attention_similar[0, 0, 2].item():.4f}\")\n",
    "\n",
    "plot_attention_pattern(attention_similar[0], title=\"Similar Q-K Pair â†’ High Attention\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  Query 0 attends strongly to Key 2 (high similarity)\")\n",
    "print(\"  This is the core mechanism: similar vectors attract attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Scaling Effects\n",
    "\n",
    "Let's visualize why scaling by âˆšd_k matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different d_k values\n",
    "d_k_values = [8, 32, 128, 512]\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "\n",
    "for idx, d_k_test in enumerate(d_k_values):\n",
    "    Q_test = torch.randn(1, 4, d_k_test)\n",
    "    K_test = torch.randn(1, 4, d_k_test)\n",
    "    V_test = torch.randn(1, 4, d_k_test)\n",
    "    \n",
    "    # Without scaling\n",
    "    scores_no_scale = Q_test @ K_test.transpose(-2, -1)\n",
    "    attn_no_scale = F.softmax(scores_no_scale, dim=-1)\n",
    "    \n",
    "    # With scaling\n",
    "    scores_with_scale = scores_no_scale / math.sqrt(d_k_test)\n",
    "    attn_with_scale = F.softmax(scores_with_scale, dim=-1)\n",
    "    \n",
    "    # Plot without scaling\n",
    "    im1 = axes[0, idx].imshow(attn_no_scale[0].detach().numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "    axes[0, idx].set_title(f'd_k={d_k_test} (no scale)')\n",
    "    if idx == 0:\n",
    "        axes[0, idx].set_ylabel('Without Scaling')\n",
    "    \n",
    "    # Plot with scaling\n",
    "    im2 = axes[1, idx].imshow(attn_with_scale[0].detach().numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "    axes[1, idx].set_title(f'd_k={d_k_test} (scaled)')\n",
    "    if idx == 0:\n",
    "        axes[1, idx].set_ylabel('With Scaling')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation:\")\n",
    "print(\"  Top row (no scaling): As d_k increases, attention becomes MORE peaked\")\n",
    "print(\"  Bottom row (scaled): Attention remains balanced regardless of d_k\")\n",
    "print(\"\\n  â†’ Scaling makes training stable across different model sizes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Real-World Example - Text Attention\n",
    "\n",
    "Let's simulate attention on actual text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a simple sentence\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "seq_len_text = len(sentence)\n",
    "d_model = 64\n",
    "\n",
    "# Create random embeddings (in reality, these come from an embedding layer)\n",
    "embeddings = torch.randn(1, seq_len_text, d_model)\n",
    "\n",
    "# For simplicity, use embeddings as Q, K, V\n",
    "Q_text = embeddings\n",
    "K_text = embeddings\n",
    "V_text = embeddings\n",
    "\n",
    "# Compute attention\n",
    "output_text, attention_text = scaled_dot_product_attention(Q_text, K_text, V_text)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_text[0].detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xticks(range(seq_len_text), sentence)\n",
    "plt.yticks(range(seq_len_text), sentence)\n",
    "plt.xlabel('Key (attending to)')\n",
    "plt.ylabel('Query (attending from)')\n",
    "plt.title('Attention Pattern for \"The cat sat on the mat\"')\n",
    "\n",
    "# Add values\n",
    "for i in range(seq_len_text):\n",
    "    for j in range(seq_len_text):\n",
    "        text = plt.text(j, i, f\"{attention_text[0, i, j].item():.2f}\",\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  Each word (row) shows how much it attends to every other word (columns)\")\n",
    "print(\"  In real models, semantically related words attend more to each other\")\n",
    "print(\"  (Our random embeddings don't have semantic meaning, so patterns are random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Causal Mask (Autoregressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply causal mask\n",
    "mask_text = create_causal_mask(seq_len_text)\n",
    "output_causal, attention_causal = scaled_dot_product_attention(\n",
    "    Q_text, K_text, V_text, mask=mask_text.unsqueeze(0)\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention_causal[0].detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xticks(range(seq_len_text), sentence)\n",
    "plt.yticks(range(seq_len_text), sentence)\n",
    "plt.xlabel('Key (past â†’ future)')\n",
    "plt.ylabel('Query')\n",
    "plt.title('Causal Attention (can only look at past)')\n",
    "\n",
    "for i in range(seq_len_text):\n",
    "    for j in range(seq_len_text):\n",
    "        text = plt.text(j, i, f\"{attention_causal[0, i, j].item():.2f}\",\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice:\")\n",
    "print(\"  'cat' can only attend to 'The' and 'cat' (past and present)\")\n",
    "print(\"  'mat' can attend to all previous words\")\n",
    "print(\"  Upper triangle is all zeros (future is blocked)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Performance and Complexity\n",
    "\n",
    "Let's measure attention's time and memory complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "seq_lengths = [16, 32, 64, 128, 256, 512]\n",
    "d_k = 64\n",
    "batch_size = 8\n",
    "\n",
    "times = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    Q = torch.randn(batch_size, seq_len, d_k)\n",
    "    K = torch.randn(batch_size, seq_len, d_k)\n",
    "    V = torch.randn(batch_size, seq_len, d_k)\n",
    "    \n",
    "    # Warmup\n",
    "    _ = scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    # Measure\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = scaled_dot_product_attention(Q, K, V)\n",
    "    elapsed = time.time() - start\n",
    "    times.append(elapsed / 100)  # Average per call\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lengths, times, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Attention Complexity: O(nÂ²)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add O(nÂ²) reference\n",
    "reference = [(s/seq_lengths[0])**2 * times[0] for s in seq_lengths]\n",
    "plt.plot(seq_lengths, reference, '--', alpha=0.5, label='O(nÂ²) reference')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  Time grows quadratically with sequence length\")\n",
    "print(\"  Doubling seq_len â†’ 4x the time\")\n",
    "print(\"  This is why transformers struggle with very long sequences!\")\n",
    "print(\"\\n  Solutions: sparse attention, linear attention, etc. (advanced topics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Interactive Experimentation\n",
    "\n",
    "Now it's your turn! Try modifying the code below to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playground - Modify these parameters and see what happens!\n",
    "\n",
    "# Parameters to experiment with\n",
    "batch_size = 1\n",
    "seq_len = 6      # Try: 4, 8, 16\n",
    "d_k = 32         # Try: 8, 16, 64, 128\n",
    "use_mask = True  # Try: True, False\n",
    "\n",
    "# Create inputs\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "# Apply attention\n",
    "if use_mask:\n",
    "    mask = create_causal_mask(seq_len).unsqueeze(0)\n",
    "    output, attention = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "    title = f\"Attention (seq_len={seq_len}, d_k={d_k}, causal)\"\n",
    "else:\n",
    "    output, attention = scaled_dot_product_attention(Q, K, V)\n",
    "    title = f\"Attention (seq_len={seq_len}, d_k={d_k}, uncausal)\"\n",
    "\n",
    "# Visualize\n",
    "plot_attention_pattern(attention[0], title=title)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Shapes: Q={Q.shape}, output={output.shape}, attention={attention.shape}\")\n",
    "\n",
    "# Experiments to try:\n",
    "print(\"\\nThings to try:\")\n",
    "print(\"  1. Set all K to be identical â†’ uniform attention\")\n",
    "print(\"  2. Make one Q-K pair very similar â†’ peaked attention\")\n",
    "print(\"  3. Increase d_k â†’ see effect on score magnitudes\")\n",
    "print(\"  4. Toggle use_mask â†’ see causal vs uncausal\")\n",
    "print(\"  5. Set V to one-hot vectors â†’ see which values are selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now explored:\n",
    "\n",
    "âœ“ **Components**: Query, Key, Value and their roles  \n",
    "âœ“ **Computation**: Q@K^T, scaling, softmax, weighted sum  \n",
    "âœ“ **Scaling**: Why âˆšd_k prevents softmax saturation  \n",
    "âœ“ **Masking**: Causal masks for autoregressive generation  \n",
    "âœ“ **Complexity**: O(nÂ²) time and space  \n",
    "âœ“ **Visualization**: Attention patterns as heatmaps  \n",
    "âœ“ **Experimentation**: Hands-on exploration  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. âœ“ Complete the exercises in `exercises/exercises.py`\n",
    "2. âœ“ Review `exercises/solutions.py` for detailed implementations\n",
    "3. âœ“ Read the test suite in `tests/test_attention.py`\n",
    "4. âœ“ Move on to Module 02: Multi-Head Attention!\n",
    "\n",
    "**Congratulations! You now understand the attention mechanism!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
