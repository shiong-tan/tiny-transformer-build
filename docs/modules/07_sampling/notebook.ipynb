{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Sampling & Generation - Interactive Notebook\n",
    "\n",
    "This notebook provides a hands-on walkthrough of text generation and sampling strategies for transformer language models.\n",
    "\n",
    "**Topics Covered:**\n",
    "1. Greedy sampling (deterministic)\n",
    "2. Temperature sampling (controlled randomness)\n",
    "3. Top-k sampling (vocabulary filtering)\n",
    "4. Top-p/nucleus sampling (dynamic filtering)\n",
    "5. Combined sampling strategies\n",
    "6. TextGenerator interface\n",
    "7. Comparison and visualization\n",
    "8. Interactive generation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent.parent))\n",
    "\n",
    "from tiny_transformer.sampling import (\n",
    "    greedy_sample,\n",
    "    temperature_sample,\n",
    "    top_k_sample,\n",
    "    top_p_sample,\n",
    "    combined_sample,\n",
    "    TextGenerator,\n",
    "    GeneratorConfig\n",
    ")\n",
    "from tiny_transformer.model import TinyTransformerLM, get_model_config\n",
    "from tiny_transformer.training import CharTokenizer, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Sampling Basics\n",
    "\n",
    "Language models output **logits** (unnormalized scores) for each token in the vocabulary. To generate text, we need to convert these logits into actual token selections.\n",
    "\n",
    "Let's start with a simple example using mock logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple example logits\n",
    "# Higher logits = higher probability after softmax\n",
    "vocab = ['the', 'cat', 'dog', 'sat', 'ran', 'jumped', 'on', 'quickly']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Example logits for next word prediction\n",
    "logits = torch.tensor([[\n",
    "    1.5,   # 'the'\n",
    "    2.8,   # 'cat' - highest logit\n",
    "    2.5,   # 'dog'\n",
    "    0.5,   # 'sat'\n",
    "    1.0,   # 'ran'\n",
    "    0.8,   # 'jumped'\n",
    "    0.3,   # 'on'\n",
    "    0.2    # 'quickly'\n",
    "]])\n",
    "\n",
    "# Convert to probabilities\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(vocab, probs[0].numpy())\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Token Probability Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Token probabilities:\")\n",
    "for token, prob in zip(vocab, probs[0]):\n",
    "    print(f\"  {token:10s}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Greedy Sampling\n",
    "\n",
    "**Greedy sampling** always picks the token with the highest probability.\n",
    "\n",
    "- **Pros:** Deterministic, reproducible\n",
    "- **Cons:** Repetitive, lacks diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy sampling: argmax\n",
    "greedy_token = greedy_sample(logits)\n",
    "print(f\"Greedy selection: {vocab[greedy_token.item()]}\")\n",
    "print(f\"Selected probability: {probs[0, greedy_token].item():.4f}\")\n",
    "\n",
    "# Test determinism\n",
    "print(\"\\nDeterminism test (10 samples):\")\n",
    "samples = [greedy_sample(logits).item() for _ in range(10)]\n",
    "print(f\"All samples identical: {len(set(samples)) == 1}\")\n",
    "print(f\"All samples: {[vocab[s] for s in samples]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temperature Sampling\n",
    "\n",
    "**Temperature** controls randomness by scaling logits before softmax:\n",
    "\n",
    "$$\\text{probs} = \\text{softmax}(\\text{logits} / T)$$\n",
    "\n",
    "- **T → 0**: Becomes greedy (deterministic)\n",
    "- **T = 1**: Original distribution\n",
    "- **T > 1**: More random (flatter distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, temp in enumerate(temperatures):\n",
    "    # Compute temperature-scaled probabilities\n",
    "    scaled_probs = F.softmax(logits / temp, dim=-1)\n",
    "    \n",
    "    axes[idx].bar(vocab, scaled_probs[0].numpy())\n",
    "    axes[idx].set_xlabel('Token')\n",
    "    axes[idx].set_ylabel('Probability')\n",
    "    axes[idx].set_title(f'Temperature = {temp}')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate entropy (measure of randomness)\n",
    "    entropy = -(scaled_probs * torch.log(scaled_probs + 1e-10)).sum().item()\n",
    "    axes[idx].text(0.02, 0.98, f'Entropy: {entropy:.3f}', \n",
    "                   transform=axes[idx].transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Lower temperature → more peaked distribution → less randomness\")\n",
    "print(\"             Higher temperature → flatter distribution → more randomness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample with different temperatures\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Temperature sampling (20 samples each):\\n\")\n",
    "for temp in [0.1, 0.5, 1.0, 2.0]:\n",
    "    samples = []\n",
    "    for _ in range(20):\n",
    "        token = temperature_sample(logits, temperature=temp)\n",
    "        samples.append(vocab[token.item()])\n",
    "    \n",
    "    # Count frequencies\n",
    "    freq = Counter(samples)\n",
    "    \n",
    "    print(f\"T = {temp}:\")\n",
    "    print(f\"  Unique tokens: {len(freq)}/{vocab_size}\")\n",
    "    print(f\"  Most common: {freq.most_common(3)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-K Sampling\n",
    "\n",
    "**Top-k sampling** filters the vocabulary to the K tokens with highest probability, then samples from them.\n",
    "\n",
    "- Prevents sampling from very low-probability tokens\n",
    "- Fixed vocabulary size K\n",
    "- Common values: K = 10, 50, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-k filtering\n",
    "k_values = [1, 2, 4, 8]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "original_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    # Get top-k tokens\n",
    "    topk_probs, topk_indices = torch.topk(original_probs, k, dim=-1)\n",
    "    \n",
    "    # Create filtered distribution (zeros for non-top-k)\n",
    "    filtered_probs = torch.zeros_like(original_probs)\n",
    "    filtered_probs.scatter_(1, topk_indices, topk_probs)\n",
    "    \n",
    "    # Renormalize\n",
    "    filtered_probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    axes[idx].bar(vocab, filtered_probs[0].numpy(), alpha=0.7, label='Top-k')\n",
    "    axes[idx].bar(vocab, original_probs[0].numpy(), alpha=0.3, label='Original')\n",
    "    axes[idx].set_xlabel('Token')\n",
    "    axes[idx].set_ylabel('Probability')\n",
    "    axes[idx].set_title(f'Top-k = {k}')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample with different k values\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Top-k sampling (20 samples each):\\n\")\n",
    "for k in [1, 2, 4, 8]:\n",
    "    samples = []\n",
    "    for _ in range(20):\n",
    "        token = top_k_sample(logits, k=k, temperature=1.0)\n",
    "        samples.append(vocab[token.item()])\n",
    "    \n",
    "    freq = Counter(samples)\n",
    "    \n",
    "    print(f\"k = {k}:\")\n",
    "    print(f\"  Unique tokens: {len(freq)}/{vocab_size}\")\n",
    "    print(f\"  Most common: {freq.most_common(3)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Top-P (Nucleus) Sampling\n",
    "\n",
    "**Top-p sampling** (nucleus sampling) selects the smallest set of tokens whose cumulative probability exceeds p.\n",
    "\n",
    "- Dynamic vocabulary size (adapts to distribution)\n",
    "- More principled than top-k\n",
    "- Common values: p = 0.9, 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-p filtering\n",
    "p_values = [0.5, 0.7, 0.9, 0.95]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "original_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "for idx, p in enumerate(p_values):\n",
    "    # Sort probabilities\n",
    "    sorted_probs, sorted_indices = torch.sort(original_probs, descending=True, dim=-1)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    # Find cutoff\n",
    "    cutoff_mask = cumulative_probs <= p\n",
    "    # Include at least one token\n",
    "    cutoff_mask[..., 0] = True\n",
    "    \n",
    "    # Filter\n",
    "    filtered_probs = torch.zeros_like(original_probs)\n",
    "    filtered_probs.scatter_(1, sorted_indices, sorted_probs * cutoff_mask.float())\n",
    "    \n",
    "    # Renormalize\n",
    "    filtered_probs = filtered_probs / (filtered_probs.sum(dim=-1, keepdim=True) + 1e-10)\n",
    "    \n",
    "    num_kept = cutoff_mask.sum().item()\n",
    "    \n",
    "    axes[idx].bar(vocab, filtered_probs[0].numpy(), alpha=0.7, label=f'Top-p (kept {num_kept})')\n",
    "    axes[idx].bar(vocab, original_probs[0].numpy(), alpha=0.3, label='Original')\n",
    "    axes[idx].set_xlabel('Token')\n",
    "    axes[idx].set_ylabel('Probability')\n",
    "    axes[idx].set_title(f'Top-p = {p} (nucleus size: {num_kept})')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Top-p adapts to the distribution shape\")\n",
    "print(\"            Peaked distributions → fewer tokens\")\n",
    "print(\"            Flat distributions → more tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample with different p values\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Top-p sampling (20 samples each):\\n\")\n",
    "for p in [0.5, 0.7, 0.9, 0.95]:\n",
    "    samples = []\n",
    "    for _ in range(20):\n",
    "        token = top_p_sample(logits, p=p, temperature=1.0)\n",
    "        samples.append(vocab[token.item()])\n",
    "    \n",
    "    freq = Counter(samples)\n",
    "    \n",
    "    print(f\"p = {p}:\")\n",
    "    print(f\"  Unique tokens: {len(freq)}/{vocab_size}\")\n",
    "    print(f\"  Most common: {freq.most_common(3)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combined Sampling\n",
    "\n",
    "**Combined sampling** applies multiple filters in sequence:\n",
    "1. Temperature scaling\n",
    "2. Top-k filtering\n",
    "3. Top-p filtering\n",
    "\n",
    "This is the most common approach in production systems (e.g., GPT, Claude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different sampling configurations\n",
    "configs = [\n",
    "    {\"name\": \"Greedy\", \"temp\": 0.0, \"k\": None, \"p\": None},\n",
    "    {\"name\": \"Low temp\", \"temp\": 0.5, \"k\": None, \"p\": None},\n",
    "    {\"name\": \"Temp + Top-k\", \"temp\": 0.8, \"k\": 4, \"p\": None},\n",
    "    {\"name\": \"Temp + Top-p\", \"temp\": 0.8, \"k\": None, \"p\": 0.9},\n",
    "    {\"name\": \"All combined\", \"temp\": 0.8, \"k\": 6, \"p\": 0.95},\n",
    "]\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Sampling strategy comparison (30 samples each):\\n\")\n",
    "for config in configs:\n",
    "    samples = []\n",
    "    for _ in range(30):\n",
    "        if config[\"temp\"] == 0.0:\n",
    "            token = greedy_sample(logits)\n",
    "        else:\n",
    "            token = combined_sample(\n",
    "                logits, \n",
    "                temperature=config[\"temp\"],\n",
    "                k=config[\"k\"],\n",
    "                p=config[\"p\"]\n",
    "            )\n",
    "        samples.append(vocab[token.item()])\n",
    "    \n",
    "    freq = Counter(samples)\n",
    "    \n",
    "    print(f\"{config['name']}:\")\n",
    "    print(f\"  Config: temp={config['temp']}, k={config['k']}, p={config['p']}\")\n",
    "    print(f\"  Unique tokens: {len(freq)}/{vocab_size}\")\n",
    "    print(f\"  Distribution: {freq.most_common()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TextGenerator - High-Level Interface\n",
    "\n",
    "The `TextGenerator` class provides a convenient interface for autoregressive text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny model for generation demos\n",
    "text = \"hello world this is a test of the text generation system. \" * 10\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = CharTokenizer()\n",
    "tokenizer.fit(text)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Characters: {sorted(tokenizer.vocab.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a tiny model (just for demo purposes)\n",
    "# In practice, you'd load a pre-trained checkpoint\n",
    "\n",
    "from tiny_transformer.training import TextDataset, Trainer, TrainerConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create tiny model\n",
    "config = get_model_config('tiny')\n",
    "model = TinyTransformerLM(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    **config\n",
    ")\n",
    "\n",
    "# Quick training (just to get sensible outputs)\n",
    "tokens = tokenizer.encode(text)\n",
    "dataset = TextDataset(tokens, seq_len=32)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    learning_rate=1e-3,\n",
    "    max_steps=100,\n",
    "    log_interval=20,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, train_loader, config=trainer_config)\n",
    "print(\"Training for 100 steps...\")\n",
    "trainer.train()\n",
    "print(\"\\n✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TextGenerator with different configurations\n",
    "model.eval()\n",
    "\n",
    "# Starting prompt\n",
    "prompt = \"hello \"\n",
    "prompt_tokens = torch.tensor([tokenizer.encode(prompt)])\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Try different sampling strategies\n",
    "strategies = [\n",
    "    {\"name\": \"Greedy\", \"config\": GeneratorConfig(max_new_tokens=30, do_sample=False)},\n",
    "    {\"name\": \"Temperature=0.5\", \"config\": GeneratorConfig(max_new_tokens=30, temperature=0.5)},\n",
    "    {\"name\": \"Temperature=1.0\", \"config\": GeneratorConfig(max_new_tokens=30, temperature=1.0)},\n",
    "    {\"name\": \"Top-k=5\", \"config\": GeneratorConfig(max_new_tokens=30, temperature=0.8, top_k=5)},\n",
    "    {\"name\": \"Top-p=0.9\", \"config\": GeneratorConfig(max_new_tokens=30, temperature=0.8, top_p=0.9)},\n",
    "    {\"name\": \"Combined\", \"config\": GeneratorConfig(max_new_tokens=30, temperature=0.8, top_k=10, top_p=0.95)},\n",
    "]\n",
    "\n",
    "for strategy in strategies:\n",
    "    generator = TextGenerator(model, strategy[\"config\"], device='cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_tokens = generator.generate(prompt_tokens)\n",
    "    \n",
    "    output_text = tokenizer.decode(output_tokens[0].tolist())\n",
    "    \n",
    "    print(f\"\\n{strategy['name']}:\")\n",
    "    print(f\"  {output_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Diversity vs Quality Trade-off\n",
    "\n",
    "Different sampling strategies offer different trade-offs between:\n",
    "- **Quality**: Coherence, grammaticality\n",
    "- **Diversity**: Variety, creativity\n",
    "\n",
    "Let's measure diversity quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_diversity(model, generator_config, prompt_tokens, tokenizer, num_samples=10):\n",
    "    \"\"\"Measure diversity of generated samples.\"\"\"\n",
    "    generator = TextGenerator(model, generator_config, device='cpu')\n",
    "    \n",
    "    texts = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            output_tokens = generator.generate(prompt_tokens)\n",
    "            text = tokenizer.decode(output_tokens[0].tolist())\n",
    "            texts.append(text)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    unique_texts = len(set(texts))\n",
    "    avg_length = np.mean([len(t) for t in texts])\n",
    "    \n",
    "    # Character-level diversity\n",
    "    all_chars = ''.join(texts)\n",
    "    unique_chars = len(set(all_chars))\n",
    "    \n",
    "    return {\n",
    "        'unique_samples': unique_texts,\n",
    "        'total_samples': num_samples,\n",
    "        'uniqueness_ratio': unique_texts / num_samples,\n",
    "        'avg_length': avg_length,\n",
    "        'unique_chars': unique_chars,\n",
    "        'samples': texts[:3]  # First 3 for inspection\n",
    "    }\n",
    "\n",
    "# Compare diversity\n",
    "configs_to_test = [\n",
    "    (\"Greedy\", GeneratorConfig(max_new_tokens=20, do_sample=False)),\n",
    "    (\"Temp=0.3\", GeneratorConfig(max_new_tokens=20, temperature=0.3)),\n",
    "    (\"Temp=0.8\", GeneratorConfig(max_new_tokens=20, temperature=0.8)),\n",
    "    (\"Temp=1.5\", GeneratorConfig(max_new_tokens=20, temperature=1.5)),\n",
    "    (\"Top-p=0.9\", GeneratorConfig(max_new_tokens=20, temperature=0.8, top_p=0.9)),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, config in configs_to_test:\n",
    "    print(f\"Testing {name}...\")\n",
    "    metrics = measure_diversity(model, config, prompt_tokens, tokenizer, num_samples=10)\n",
    "    results.append((name, metrics))\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIVERSITY ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for name, metrics in results:\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Unique samples: {metrics['unique_samples']}/{metrics['total_samples']}\")\n",
    "    print(f\"  Uniqueness ratio: {metrics['uniqueness_ratio']:.2%}\")\n",
    "    print(f\"  Unique characters: {metrics['unique_chars']}\")\n",
    "    print(f\"  Sample outputs:\")\n",
    "    for i, sample in enumerate(metrics['samples'], 1):\n",
    "        print(f\"    {i}. {sample[:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize diversity vs temperature\n",
    "temperatures = [0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5]\n",
    "diversity_scores = []\n",
    "\n",
    "for temp in temperatures:\n",
    "    config = GeneratorConfig(max_new_tokens=20, temperature=temp)\n",
    "    metrics = measure_diversity(model, config, prompt_tokens, tokenizer, num_samples=20)\n",
    "    diversity_scores.append(metrics['uniqueness_ratio'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(temperatures, diversity_scores, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Uniqueness Ratio')\n",
    "plt.title('Diversity vs Temperature')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=1.0, color='g', linestyle='--', alpha=0.5, label='Perfect diversity')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Higher temperature → more diversity (but possibly less coherence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. EOS Token Handling\n",
    "\n",
    "TextGenerator can stop generation early when an end-of-sequence (EOS) token is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate EOS handling\n",
    "# Let's use newline as EOS\n",
    "eos_token_id = tokenizer.vocab.get('\\n', None)\n",
    "\n",
    "if eos_token_id is not None:\n",
    "    print(f\"Using newline as EOS token (id={eos_token_id})\\n\")\n",
    "    \n",
    "    # Generate with EOS handling\n",
    "    config = GeneratorConfig(\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.8,\n",
    "        eos_token_id=eos_token_id\n",
    "    )\n",
    "    \n",
    "    generator = TextGenerator(model, config, device='cpu')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_tokens = generator.generate(prompt_tokens)\n",
    "    \n",
    "    output_text = tokenizer.decode(output_tokens[0].tolist())\n",
    "    \n",
    "    print(f\"Generated text (stops at newline):\")\n",
    "    print(f\"  {output_text}\")\n",
    "    print(f\"\\nGenerated {len(output_tokens[0])} tokens (max was {len(prompt_tokens[0]) + 50})\")\n",
    "else:\n",
    "    print(\"No newline in vocabulary, skipping EOS demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Batch Generation\n",
    "\n",
    "TextGenerator supports generating multiple sequences in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple prompts\n",
    "prompts = [\n",
    "    \"hello \",\n",
    "    \"this is \",\n",
    "    \"test \"\n",
    "]\n",
    "\n",
    "prompt_tokens_batch = torch.tensor([tokenizer.encode(p) for p in prompts])\n",
    "print(f\"Batch shape: {prompt_tokens_batch.shape}\\n\")\n",
    "\n",
    "# Generate for all prompts at once\n",
    "config = GeneratorConfig(\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "generator = TextGenerator(model, config, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_tokens_batch = generator.generate(prompt_tokens_batch)\n",
    "\n",
    "print(\"Batch generation results:\\n\")\n",
    "for i, (prompt, output_tokens) in enumerate(zip(prompts, output_tokens_batch)):\n",
    "    output_text = tokenizer.decode(output_tokens.tolist())\n",
    "    print(f\"{i+1}. Prompt: '{prompt}'\")\n",
    "    print(f\"   Output: '{output_text}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "1. **Greedy Sampling** - Deterministic, always picks highest probability token\n",
    "2. **Temperature Sampling** - Controls randomness by scaling logits\n",
    "3. **Top-K Sampling** - Filters to K highest probability tokens\n",
    "4. **Top-P (Nucleus) Sampling** - Dynamically filters by cumulative probability\n",
    "5. **Combined Sampling** - Uses temperature + top-k + top-p together\n",
    "6. **TextGenerator** - High-level interface for text generation\n",
    "7. **Diversity Analysis** - Trade-offs between quality and diversity\n",
    "8. **EOS Handling** - Early stopping with end-of-sequence tokens\n",
    "9. **Batch Generation** - Efficient parallel generation\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Greedy** is fast but repetitive\n",
    "- **Temperature** controls randomness (lower = more deterministic)\n",
    "- **Top-k** prevents low-probability tokens (fixed size)\n",
    "- **Top-p** adapts to distribution shape (variable size)\n",
    "- **Combined** (temp + top-k/top-p) is most common in production\n",
    "- Higher diversity often means lower coherence\n",
    "\n",
    "### Recommended Settings:\n",
    "\n",
    "- **Factual text**: temperature=0.3-0.5, top-p=0.9\n",
    "- **Creative writing**: temperature=0.8-1.0, top-p=0.95\n",
    "- **Code generation**: temperature=0.2-0.4, top-p=0.9\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try the exercises in `exercises/exercises.py`\n",
    "- Experiment with different sampling strategies\n",
    "- Train on larger datasets (Shakespeare, WikiText)\n",
    "- Move on to Module 08 (Engineering Practices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
