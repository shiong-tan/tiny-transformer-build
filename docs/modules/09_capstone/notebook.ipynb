{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shakespeare Transformer - Interactive Capstone\n",
        "\n",
        "Complete end-to-end training of a character-level transformer on Shakespeare text.\n",
        "\n",
        "**What You'll Learn:**\n",
        "- Data preparation and tokenization\n",
        "- Model configuration and training\n",
        "- Text generation with different sampling strategies\n",
        "- Model evaluation and analysis\n",
        "\n",
        "**Time**: ~10-15 minutes (on GPU/Colab with reduced steps)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/tiny-transformer-course/blob/main/docs/modules/09_capstone/notebook.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation\n",
        "\n",
        "Install dependencies and setup environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify PyTorch\n",
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "print(f'Python: {sys.version}')\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "print(f'Device: {\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'}')\n",
        "\n",
        "# Add project to path (if running locally)\n",
        "if Path('../../../tiny_transformer').exists():\n",
        "    sys.path.insert(0, str(Path.cwd().parent.parent.parent))\n",
        "    print('✓ Running locally')\n",
        "else:\n",
        "    print('Note: Adjust paths for your environment')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Download and analyze the Shakespeare dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Shakespeare dataset\n",
        "data_path = '../../../data/tiny_shakespeare.txt'\n",
        "\n",
        "with open(data_path, 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f'Total characters: {len(text):,}')\n",
        "print(f'Total lines: {text.count(chr(10)):,}')\n",
        "print(f'Vocabulary size: {len(set(text))}')\n",
        "print(f'\\nFirst 500 characters:')\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character distribution analysis\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "char_counts = Counter(text)\n",
        "top_chars = char_counts.most_common(20)\n",
        "\n",
        "chars = [repr(c) if c in ['\\n', '\\t', ' '] else c for c, _ in top_chars]\n",
        "counts = [count for _, count in top_chars]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(chars, counts)\n",
        "plt.xlabel('Character')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Characters in Shakespeare Dataset')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenization\n",
        "\n",
        "Create character-level tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tiny_transformer.training import CharTokenizer\n",
        "\n",
        "# Create and fit tokenizer\n",
        "tokenizer = CharTokenizer()\n",
        "tokenizer.fit(text)\n",
        "\n",
        "print(f'Vocabulary size: {tokenizer.vocab_size}')\n",
        "print(f'\\nFirst 10 vocab items:')\n",
        "for char, idx in list(tokenizer.vocab.items())[:10]:\n",
        "    print(f'  {repr(char)}: {idx}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test encoding/decoding\n",
        "test_text = 'ROMEO: What say you?'\n",
        "tokens = tokenizer.encode(test_text)\n",
        "decoded = tokenizer.decode(tokens)\n",
        "\n",
        "print(f'Original: {test_text}')\n",
        "print(f'Tokens: {tokens}')\n",
        "print(f'Decoded: {decoded}')\n",
        "print(f'Match: {test_text == decoded}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Configuration\n",
        "\n",
        "Setup model and training configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tiny_transformer.model import TinyTransformerLM\n",
        "\n",
        "# Model configuration (smaller for faster training)\n",
        "config = {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'n_heads': 4,\n",
        "    'n_layers': 4,\n",
        "    'd_ff': 1024,\n",
        "    'max_seq_len': 128,\n",
        "    'dropout': 0.1,\n",
        "    'tie_weights': True,\n",
        "}\n",
        "\n",
        "# Create model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = TinyTransformerLM(**config).to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'Model parameters: {num_params:,}')\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Setup\n",
        "\n",
        "Prepare datasets and training components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tiny_transformer.training import TextDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Tokenize full text\n",
        "tokens = tokenizer.encode(text)\n",
        "\n",
        "# Split train/val (90/10)\n",
        "val_size = len(tokens) // 10\n",
        "train_tokens = tokens[:-val_size]\n",
        "val_tokens = tokens[-val_size:]\n",
        "\n",
        "print(f'Train tokens: {len(train_tokens):,}')\n",
        "print(f'Val tokens: {len(val_tokens):,}')\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TextDataset(train_tokens, seq_len=128, stride=64)\n",
        "val_dataset = TextDataset(val_tokens, seq_len=128, stride=128)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f'Train batches: {len(train_loader)}')\n",
        "print(f'Val batches: {len(val_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tiny_transformer.training import Trainer, TrainerConfig, WarmupCosineScheduler\n",
        "\n",
        "# Training configuration\n",
        "trainer_config = TrainerConfig(\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=0.01,\n",
        "    grad_clip=1.0,\n",
        "    warmup_steps=200,\n",
        "    max_steps=2000,\n",
        "    eval_interval=200,\n",
        "    log_interval=50,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(model, train_loader, val_loader, trainer_config)\n",
        "\n",
        "# Setup scheduler\n",
        "scheduler = WarmupCosineScheduler(\n",
        "    optimizer=trainer.optimizer,\n",
        "    warmup_steps=trainer_config.warmup_steps,\n",
        "    total_steps=trainer_config.max_steps,\n",
        "    peak_lr=trainer_config.learning_rate,\n",
        "    min_lr=1e-5,\n",
        ")\n",
        "trainer.scheduler = scheduler\n",
        "\n",
        "print('Training setup complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training\n",
        "\n",
        "Train the model (this will take ~10-15 minutes on GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train with progress tracking\n",
        "import time\n",
        "\n",
        "print('='*70)\n",
        "print('Starting Training')\n",
        "print('='*70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    metrics = trainer.train()\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    print(f'\\nTraining complete in {elapsed/60:.1f} minutes')\n",
        "    print(f'Final train loss: {metrics.get(\"train_loss\", 0):.3f}')\n",
        "    print(f'Final val loss: {metrics.get(\"val_loss\", 0):.3f}')\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print('\\nTraining interrupted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Text Generation\n",
        "\n",
        "Generate Shakespeare-style text with the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tiny_transformer.sampling import TextGenerator, GeneratorConfig\n",
        "\n",
        "def generate(prompt, temperature=0.8, max_tokens=150):\n",
        "    \"\"\"Generate text from prompt.\"\"\"\n",
        "    gen_config = GeneratorConfig(\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    \n",
        "    generator = TextGenerator(model, gen_config, device=device)\n",
        "    \n",
        "    model.eval()\n",
        "    prompt_tokens = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output_tokens = generator.generate(prompt_tokens)\n",
        "    \n",
        "    return tokenizer.decode(output_tokens[0].tolist())\n",
        "\n",
        "# Generate sample\n",
        "prompt = 'ROMEO:'\n",
        "generated = generate(prompt)\n",
        "\n",
        "print('='*70)\n",
        "print(f'Prompt: {prompt}')\n",
        "print('-'*70)\n",
        "print(generated)\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Compare Sampling Strategies\n",
        "\n",
        "Experiment with different sampling parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different temperatures\n",
        "prompt = 'HAMLET:'\n",
        "temperatures = [0.5, 0.8, 1.2]\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f'\\nTemperature: {temp}')\n",
        "    print('-'*70)\n",
        "    generated = generate(prompt, temperature=temp, max_tokens=100)\n",
        "    print(generated)\n",
        "    print('-'*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Interactive Generation\n",
        "\n",
        "Try your own prompts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive cell - run multiple times with different prompts\n",
        "your_prompt = 'JULIET:'  # Change this!\n",
        "temperature = 0.8  # Adjust creativity\n",
        "max_tokens = 200  # Length\n",
        "\n",
        "result = generate(your_prompt, temperature, max_tokens)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Model\n",
        "\n",
        "Save the trained model for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save checkpoint\n",
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'config': config,\n",
        "    'tokenizer_vocab': tokenizer.vocab,\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, 'shakespeare_model.pt')\n",
        "print('✓ Model saved to shakespeare_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Congratulations!\n",
        "\n",
        "You've successfully trained a character-level transformer on Shakespeare!\n",
        "\n",
        "**What you learned:**\n",
        "- Data preparation and tokenization\n",
        "- Model architecture and configuration\n",
        "- Training loop with validation\n",
        "- Text generation with sampling strategies\n",
        "\n",
        "**Next steps:**\n",
        "1. Train longer (20k steps) for better quality\n",
        "2. Try different model sizes (d_model=384, n_layers=6)\n",
        "3. Fine-tune on your own text data\n",
        "4. Experiment with advanced sampling (beam search, nucleus sampling)\n",
        "\n",
        "See `docs/modules/09_capstone/walkthrough.md` for the complete guide!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {"name": "ipython", "version": 3},
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
