{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Transformer Block - Interactive Exploration\n",
    "\n",
    "This notebook provides hands-on exploration of the transformer block, the core building block of transformer models.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how feed-forward networks transform sequences\n",
    "- Explore layer normalization and residual connections\n",
    "- Compare Pre-LN vs Post-LN architectures\n",
    "- Visualize gradient flow through deep stacks\n",
    "- Analyze attention patterns through multiple blocks\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed Module 01 (Attention)\n",
    "- Completed Module 02 (Multi-Head Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../../../')  # Add repo root to path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tiny_transformer.feedforward import FeedForward\n",
    "from tiny_transformer.transformer_block import TransformerBlock\n",
    "from tiny_transformer.multi_head import MultiHeadAttention\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feed-Forward Networks\n",
    "\n",
    "The feed-forward network (FFN) is applied to each position independently. It expands the representation to a higher dimension (`d_ff`), applies an activation, and projects back.\n",
    "\n",
    "**Architecture:** `d_model → d_ff → d_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feed-forward network\n",
    "d_model = 256\n",
    "d_ff = 1024  # Typically 4× d_model\n",
    "\n",
    "ff = FeedForward(d_model=d_model, d_ff=d_ff, dropout=0.0)\n",
    "\n",
    "# Test input\n",
    "batch_size, seq_len = 8, 32\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output = ff(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nShape preserved: {x.shape == output.shape}\")\n",
    "print(f\"\\nParameter count: {sum(p.numel() for p in ff.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FFN transformation\n",
    "ff.eval()\n",
    "x_single = torch.randn(1, 10, d_model)\n",
    "y_single = ff(x_single)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Input\n",
    "axes[0].imshow(x_single[0].detach().numpy(), aspect='auto', cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[0].set_title('Input (seq_len, d_model)')\n",
    "axes[0].set_xlabel('d_model')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].colorbar()\n",
    "\n",
    "# Output\n",
    "axes[1].imshow(y_single[0].detach().numpy(), aspect='auto', cmap='RdBu', vmin=-2, vmax=2)\n",
    "axes[1].set_title('Output (seq_len, d_model)')\n",
    "axes[1].set_xlabel('d_model')\n",
    "axes[1].set_ylabel('Position')\n",
    "axes[1].colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feed-forward network transforms each position independently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Complete Transformer Block\n",
    "\n",
    "The transformer block combines:\n",
    "1. **Multi-head self-attention** - Allows positions to communicate\n",
    "2. **Feed-forward network** - Non-linear transformations\n",
    "3. **Layer normalization** - Stabilizes training\n",
    "4. **Residual connections** - Enables deep networks\n",
    "\n",
    "We use **Pre-LN** architecture (modern standard): normalize *before* each sublayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer block\n",
    "d_model = 256\n",
    "n_heads = 8\n",
    "d_ff = 1024\n",
    "\n",
    "block = TransformerBlock(\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(8, 32, d_model)\n",
    "output, attn_weights = block(x, return_attention=True)\n",
    "\n",
    "print(f\"Input shape:            {x.shape}\")\n",
    "print(f\"Output shape:           {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nTotal parameters:       {sum(p.numel() for p in block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns through block\n",
    "block.eval()\n",
    "x_vis = torch.randn(1, 16, d_model)\n",
    "output_vis, attn_vis = block(x_vis, return_attention=True)\n",
    "\n",
    "# Plot attention for first few heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(min(8, n_heads)):\n",
    "    ax = axes[head_idx]\n",
    "    attn_head = attn_vis[0, head_idx].detach().numpy()\n",
    "    \n",
    "    im = ax.imshow(attn_head, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {head_idx + 1}')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('Attention Patterns Across Heads', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Residual Connections and Gradient Flow\n",
    "\n",
    "Residual connections are critical for training deep networks. They create a \"gradient highway\" that allows gradients to flow directly from output to input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gradient flow: with vs without residuals\n",
    "def measure_gradient_flow(model, depth=6):\n",
    "    \"\"\"Measure gradient norms at each layer.\"\"\"\n",
    "    x = torch.randn(8, 32, d_model, requires_grad=True)\n",
    "    \n",
    "    # Forward through stack\n",
    "    activations = [x]\n",
    "    for i in range(depth):\n",
    "        x, _ = model(x)\n",
    "        activations.append(x)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss = x.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Measure gradients\n",
    "    grad_norms = []\n",
    "    for act in activations:\n",
    "        if act.grad is not None:\n",
    "            grad_norms.append(act.grad.norm().item())\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "# Test with transformer block (has residuals)\n",
    "block_with_residuals = TransformerBlock(d_model=256, n_heads=8, d_ff=1024, dropout=0.0)\n",
    "grads_with = measure_gradient_flow(block_with_residuals, depth=6)\n",
    "\n",
    "# Plot gradient flow\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(grads_with, marker='o', linewidth=2, markersize=8, label='With Residuals')\n",
    "plt.xlabel('Layer Depth')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.title('Gradient Flow Through Deep Stack')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Residual connections maintain gradient flow even in deep networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Pre-LN vs Post-LN Architecture\n",
    "\n",
    "**Pre-LN (Modern):** Normalize *before* sublayers  \n",
    "**Post-LN (Original):** Normalize *after* sublayers\n",
    "\n",
    "Pre-LN provides better training stability for deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation norms through deep stack\n",
    "def measure_activation_norms(block, depth=12):\n",
    "    \"\"\"Measure activation norms at each layer.\"\"\"\n",
    "    block.eval()\n",
    "    x = torch.randn(16, 64, d_model)\n",
    "    \n",
    "    norms = [x.norm(dim=-1).mean().item()]\n",
    "    \n",
    "    for i in range(depth):\n",
    "        x, _ = block(x)\n",
    "        norms.append(x.norm(dim=-1).mean().item())\n",
    "    \n",
    "    return norms\n",
    "\n",
    "# Pre-LN (our implementation)\n",
    "block_preln = TransformerBlock(d_model=256, n_heads=8, d_ff=1024, dropout=0.0)\n",
    "norms_preln = measure_activation_norms(block_preln, depth=12)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(norms_preln, marker='o', linewidth=2, markersize=8, label='Pre-LN')\n",
    "plt.xlabel('Layer Depth')\n",
    "plt.ylabel('Activation Norm')\n",
    "plt.title('Activation Norms Through Deep Stack')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Pre-LN maintains stable norms: {norms_preln[0]:.2f} → {norms_preln[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Stacking Transformer Blocks\n",
    "\n",
    "Real transformer models stack multiple blocks to create deep networks. Each block can learn different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stack of transformer blocks\n",
    "num_layers = 6\n",
    "d_model = 256\n",
    "\n",
    "transformer_stack = nn.ModuleList([\n",
    "    TransformerBlock(d_model=d_model, n_heads=8, d_ff=1024, dropout=0.0)\n",
    "    for _ in range(num_layers)\n",
    "])\n",
    "\n",
    "# Forward pass through stack\n",
    "x = torch.randn(8, 32, d_model)\n",
    "attention_weights_per_layer = []\n",
    "\n",
    "for i, block in enumerate(transformer_stack):\n",
    "    x, attn = block(x, return_attention=True)\n",
    "    attention_weights_per_layer.append(attn)\n",
    "    print(f\"Layer {i+1}: output shape = {x.shape}\")\n",
    "\n",
    "print(f\"\\nFinal output shape: {x.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in transformer_stack.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how attention patterns change across layers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for layer_idx in range(min(6, num_layers)):\n",
    "    ax = axes[layer_idx]\n",
    "    \n",
    "    # Average attention across heads and batch\n",
    "    avg_attn = attention_weights_per_layer[layer_idx].mean(dim=(0, 1)).detach().numpy()\n",
    "    \n",
    "    im = ax.imshow(avg_attn, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Layer {layer_idx + 1}')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('Attention Patterns Across Transformer Layers', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Different layers learn different attention patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Causal Masking Through Blocks\n",
    "\n",
    "For autoregressive language modeling, we need to prevent positions from attending to future positions using a causal mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create causal mask\n",
    "seq_len = 16\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "# Visualize mask\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(causal_mask.numpy(), cmap='Greys', vmin=0, vmax=1)\n",
    "plt.title('Causal Mask (Lower Triangular)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Test with mask\n",
    "block = TransformerBlock(d_model=128, n_heads=4, d_ff=512, dropout=0.0)\n",
    "block.eval()\n",
    "\n",
    "x = torch.randn(1, seq_len, 128)\n",
    "output, attn = block(x, mask=causal_mask, return_attention=True)\n",
    "\n",
    "# Visualize masked attention\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Show first head\n",
    "axes[0].imshow(attn[0, 0].detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_title('Attention with Causal Mask (Head 1)')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "# Show average across heads\n",
    "avg_attn = attn[0].mean(dim=0).detach().numpy()\n",
    "axes[1].imshow(avg_attn, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[1].set_title('Average Attention Across Heads')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Causal mask successfully prevents attending to future positions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Parameter Distribution Analysis\n",
    "\n",
    "Let's analyze where parameters are distributed in a transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters by component\n",
    "block = TransformerBlock(d_model=512, n_heads=8, d_ff=2048)\n",
    "\n",
    "param_counts = {\n",
    "    'Self-Attention': sum(p.numel() for p in block.self_attn.parameters()),\n",
    "    'Feed-Forward': sum(p.numel() for p in block.feed_forward.parameters()),\n",
    "    'LayerNorm': sum(p.numel() for p in block.norm1.parameters()) + \n",
    "                 sum(p.numel() for p in block.norm2.parameters())\n",
    "}\n",
    "\n",
    "total_params = sum(param_counts.values())\n",
    "\n",
    "print(\"Parameter Distribution:\")\n",
    "print(\"-\" * 50)\n",
    "for name, count in param_counts.items():\n",
    "    percentage = (count / total_params) * 100\n",
    "    print(f\"{name:20s}: {count:>10,} ({percentage:>5.1f}%)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Total':20s}: {total_params:>10,} (100.0%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(param_counts.keys(), param_counts.values())\n",
    "plt.ylabel('Parameter Count')\n",
    "plt.title('Parameter Distribution in Transformer Block')\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you've explored:\n",
    "\n",
    "1. ✅ **Feed-Forward Networks** - Position-wise transformations\n",
    "2. ✅ **Complete Transformer Block** - Integration of all components\n",
    "3. ✅ **Residual Connections** - Gradient flow in deep networks\n",
    "4. ✅ **Pre-LN Architecture** - Training stability\n",
    "5. ✅ **Stacked Blocks** - Building deep transformers\n",
    "6. ✅ **Causal Masking** - Autoregressive modeling\n",
    "7. ✅ **Parameter Analysis** - Understanding model size\n",
    "\n",
    "**Next Steps:**\n",
    "- Complete the exercises in `exercises.py`\n",
    "- Read the theory document for deeper mathematical understanding\n",
    "- Move to Module 04: Embeddings & Positional Encoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
